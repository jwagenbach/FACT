{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from captum.attr import GradientShap\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from lfxai.explanations.features import attribute_auxiliary\n",
    "from lfxai.models.images import ClassifierMnist, EncoderMnist\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "torch.random.manual_seed(123)\n",
    "batch_size = 128\n",
    "device = 'cpu'\n",
    "\n",
    "# Model Args\n",
    "image_height = 28\n",
    "dim_latent = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data loading\n",
    "data_dir = \"data/mnist\"\n",
    "shared_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = MNIST(data_dir, train=True, download=True, transform=shared_transform\n",
    "                                           )\n",
    "test_dataset = MNIST(data_dir, train=False, download=True, transform=shared_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Model loading ######\n",
    "# Specification\n",
    "dim_latent = 4\n",
    "name = \"TestClassifier\"\n",
    "classifier_state_dict_path = os.path.join('..', 'TrainedModels', 'MNIST', 'Classifier_run0.pt') # TODO Need to loop over runs\n",
    "\n",
    "# Load\n",
    "encoder = EncoderMnist(dim_latent)\n",
    "\n",
    "classifier = ClassifierMnist(encoder, dim_latent, name)\n",
    "classifier.load_state_dict(torch.load(classifier_state_dict_path), strict=True)\n",
    "\n",
    "encoder = copy.deepcopy(classifier.encoder) # Necessary? Should check whether the load just modifies the original\n",
    "\n",
    "print(\"Classifier Loaded\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate GradShap For Encoder and Full Model\n",
    "gradshap_encoder = GradientShap(encoder)\n",
    "gradshap_full_model = GradientShap(classifier)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_image = torch.zeros((1, 1, 28, 28), device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder_attributions = attribute_auxiliary(\n",
    "                        encoder, test_loader, device, gradshap_encoder, baseline_image\n",
    "                    )\n",
    "\n",
    "# Note that this is the correct thing to do here because the classifier outputs probabilities so we are taking a soft sum\n",
    "pipeline_attributions = attribute_auxiliary(\n",
    "    classifier, test_loader, device, gradshap_full_model, baseline_image\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cast each one to absolute value, since we're not interested in the direction on the hidden space\n",
    "encoder_attributions = np.abs(encoder_attributions)\n",
    "pipeline_attributions = np.abs(pipeline_attributions)\n",
    "\n",
    "# Normalise each one to have variance 1\n",
    "encoder_attributions = encoder_attributions/np.std(encoder_attributions)\n",
    "pipeline_attributions = pipeline_attributions/np.std(pipeline_attributions)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharey='row', figsize=[15, 6])\n",
    "ax[0].imshow(encoder_attributions.mean(axis=0).squeeze(), cmap='gray_r')\n",
    "ax[0].set_title('Encoder Saliency Map')\n",
    "\n",
    "ax[1].imshow(pipeline_attributions.mean(axis=0).squeeze(), cmap='gray_r')\n",
    "ax[1].set_title('Classifier Saliency Map')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# As above, but shows more detail so that we can understand what's going on better\n",
    "fig, ax = plt.subplots(figsize=[12, 12])\n",
    "ax.imshow(encoder_attributions.mean(axis=0).squeeze(), cmap='gray_r')\n",
    "for (i, j), pixel_value in np.ndenumerate(np.mean(encoder_attributions, axis=0).squeeze().round(2)):\n",
    "    ax.text(i, j, pixel_value, c='red', ha='center',va='center')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute the pearson correlation between the maps\n",
    "# TODO we should also exclude constant/nearly constant pixels?\n",
    "\n",
    "# Note there are a few ways we could possibly do this, e.g. taking the mean of the feature values first\n",
    "# However, I think that unnecessarily discards structure that we might otherwise want to keep.\n",
    "pearsons_rho = np.corrcoef(encoder_attributions.flatten(),\n",
    "                           pipeline_attributions.flatten())\n",
    "pearsons_rho"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Alternatively, we may want to consider the R**2 value. Assuming a linear relationship + intercept, this is just the\n",
    "# Square of the pearson coefficient.\n",
    "pearsons_rho**2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally we can look at what happens when we first aggregate across the images\n",
    "pearsons_rho_agg = np.corrcoef(encoder_attributions.mean(axis=0).flatten(), pipeline_attributions.mean(axis=0).flatten())\n",
    "pearsons_rho_agg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dots = (encoder_attributions*pipeline_attributions).mean(axis=0).squeeze()\n",
    "dots = dots[np.abs(dots) > 0]\n",
    "\n",
    "plt.hist(dots.flatten())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow((encoder_attributions*pipeline_attributions).mean(axis=0).squeeze(), cmap='gray_r')\n",
    "plt.colorbar()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have something of a puzzle - why do the correlations skyrocket when we first aggregate across the dimensions?\n",
    "\n",
    "To solve this, let's examine what happens in the case of a _single_ image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 2\n",
    "\n",
    "X, y = test_dataset[i]\n",
    "X = X.squeeze()\n",
    "a_enc_i = encoder_attributions[i].squeeze()\n",
    "a_full_i = pipeline_attributions[i].squeeze()\n",
    "\n",
    "#  Compute similarity in this image\n",
    "(np.corrcoef(a_enc_i.flatten(), a_full_i.flatten()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove the black pixels since these are trivially the same\n",
    "mask = (X == 0)\n",
    "m_enc = a_enc_i[~mask]\n",
    "m_full = a_full_i[~mask]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.corrcoef(m_full.flatten(), m_enc.flatten())**2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearmanr(m_full.flatten(), m_enc.flatten())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharey='row', figsize=[15, 6])\n",
    "ax[0].imshow(a_enc_i, cmap='gray_r')\n",
    "ax[0].set_title('Encoder Saliency Map')\n",
    "\n",
    "ax[1].imshow(a_full_i, cmap='gray_r')\n",
    "ax[1].set_title('Classifier Saliency Map')\n",
    "\n",
    "ax[2].imshow(X, cmap='gray_r')\n",
    "ax[2].set_title(f'Original Image - class {y}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's look for whether these are correlated acros s a _single_ image by visualising them as a scatter\n",
    "plt.scatter(m_enc, m_full)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have some satisfying answers:\n",
    "\n",
    "1. Correlations appear much lower _within_ images than across images. Intuitively, if we take the mean across the images, the bits in the centre will all end up with some feature importance between 0 and 1. However, most explainability metrics are defined per prediction, and this can be important in e.g. regulatory contexts.\n",
    "\n",
    "2. At least some of the correlation is driven by the fact that the black pixels have spuriously high correlations. Masking out these pixels makes a substantive difference.\n",
    "\n",
    "TODO: we should check whether their pearson results change when masking out some pixels. We should also check whether their results are taken per image or somehow aggregating across images.\n",
    "\n",
    "Now, we should make the above into a function that we can easily loop over a dataset, which should give us pearson correlations per image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def image_pearson(i: int, dataset, encoder_attributions, full_attributions, mask_dead_pixels=True):\n",
    "    X, y = dataset[i]\n",
    "    X = X.squeeze()\n",
    "\n",
    "    X = X.squeeze()\n",
    "    a_enc_i = encoder_attributions[i].squeeze()\n",
    "    a_full_i = full_attributions[i].squeeze()\n",
    "\n",
    "    if mask_dead_pixels:\n",
    "        mask = (X == 0)\n",
    "        a_enc_i = a_enc_i[~mask]\n",
    "        a_full_i = a_full_i[~mask]\n",
    "\n",
    "    rho = np.corrcoef(a_enc_i.flatten(), a_full_i.flatten())[0, 1]\n",
    "\n",
    "    return rho\n",
    "\n",
    "rhos = []\n",
    "for i in range(len(test_dataset)):\n",
    "    rho = image_pearson(i, test_dataset, encoder_attributions, pipeline_attributions, mask_dead_pixels=True)\n",
    "    rhos.append(rho)\n",
    "\n",
    "rhos = np.array(rhos)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.Series(rhos).mean()**2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(rhos, bins=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Further extensions\n",
    "\n",
    "Can we figure out what properties of the map make this happen? E.g. is it an issue of scale? To test this, we can look at properties of the decoder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lfxai.explanations.features import AuxiliaryFunction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decoder = copy.deepcopy(classifier.lin_output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pass through once to get a baseline for the hidden\n",
    "hiddens = []\n",
    "\n",
    "for X,y in test_loader:\n",
    "    hidden = encoder(X).detach().numpy()\n",
    "    hiddens.append(hidden)\n",
    "\n",
    "\n",
    "hiddens = np.concatenate(hiddens)\n",
    "hidden_baseline = hiddens.mean(axis=0)\n",
    "hidden_baseline = hidden_baseline[np.newaxis, ...]\n",
    "hidden_baseline = torch.Tensor(hidden_baseline)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decoder_attributions = []\n",
    "# hidden_baseline = torch.zeros(1, 4)\n",
    "\n",
    "gradshap_decoder = GradientShap(decoder)\n",
    "\n",
    "for X, y in test_loader:\n",
    "    hidden = encoder(X)\n",
    "\n",
    "    auxiliary_encoder = AuxiliaryFunction(decoder, hidden)\n",
    "    gradshap_decoder.forward_func = auxiliary_encoder\n",
    "\n",
    "    attr = gradshap_decoder.attribute(hidden, baselines=hidden_baseline)\n",
    "    attr = attr.detach().cpu().numpy()\n",
    "\n",
    "    decoder_attributions.append(attr)\n",
    "\n",
    "decoder_attributions = np.concatenate(decoder_attributions)\n",
    "decoder_attributions = np.abs(decoder_attributions)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(decoder_attributions)\n",
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now that we have obtained the decoder attributions, we need to hone in on a pixel that failed our test before and try and understand what went wrong.\n",
    "\n",
    "Note: this ended up being a bit of a failure - it seems like the scale is indeed the key, but since the explanations are locally linear, this is a pretty trivial observation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "i = 0\n",
    "\n",
    "X, y = test_dataset[i]\n",
    "X = X.squeeze()\n",
    "a_enc_i = encoder_attributions[i].squeeze()\n",
    "a_full_i = pipeline_attributions[i].squeeze()\n",
    "a_dec_i = decoder_attributions[i].squeeze()\n",
    "\n",
    "#  Compute similarity in this image\n",
    "(np.corrcoef(a_enc_i.flatten(), a_full_i.flatten()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=[15, 4])\n",
    "ax[0].imshow(a_enc_i, cmap='gray')\n",
    "ax[0].set_title('encoder')\n",
    "\n",
    "ax[1].imshow(a_full_i, cmap='gray')\n",
    "ax[1].set_title('full')\n",
    "\n",
    "ax[2].imshow(a_full_i*a_enc_i, cmap='gray')\n",
    "ax[2].set_title('corr')\n",
    "\n",
    "k, l = 16, 17\n",
    "for i in range(len(ax)):\n",
    "    ax[i].text(k, l, 'h', c='red', ha='center',va='center')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(a_full_i*a_enc_i)[k, l]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask = X > 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_corrs = pd.Series((a_full_i*a_enc_i)[mask])\n",
    "all_corrs = all_corrs.sort_values()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(all_corrs > (a_full_i*a_enc_i)[k, l]).sum()/len(all_corrs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a_full_i[k, l]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a_enc_i[k, l]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "for i in range(4):\n",
    "    a = ax.flatten()[i]\n",
    "    a.hist(df[i])\n",
    "    a.set_title(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(X, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a_dec_i"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
